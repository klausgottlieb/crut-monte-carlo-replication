{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/klausgottlieb/crut-monte-carlo-replication/blob/main/notebook_06_robustness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-markdown-header"
   },
   "source": "# Notebook 06 \u2014 Return Model Robustness\n## *When Do Charitable Remainder Unitrusts Outperform? A Monte Carlo Analysis*\n### Klaus Gottlieb, JD, MS, MBA \u2014 Wealth Care Lawyer, Cayucos, CA\n\n---\n\n## Purpose\n\nEvery Monte Carlo study rests on assumptions about the return-generating process. The baseline model uses **independent log-normally distributed annual returns** with arithmetic mean \u03bc = 7% and standard deviation \u03c3 = 12%. These are reasonable for a diversified equity-oriented portfolio over long horizons, but they are assumptions \u2014 not facts.\n\nThis notebook tests whether the core findings of the analysis survive when those assumptions are relaxed. Six robustness checks are performed:\n\n### 1. Path Count Convergence\nThe analysis series uses 10,000 simulation paths as the standard. This section documents the convergence justification: we show win probability and 95% CI width as a function of path count from 200 to 10,000 to demonstrate that results stabilize well before 10,000 and that the choice is conservative relative to what convergence requires.\n\n### 2. Volatility Sweep\n\u03c3 ranked #7 in the OAT sensitivity. The baseline \u03c3 = 12% is appropriate for a diversified portfolio, but the clients most likely to contribute to a CRUT hold **concentrated positions** \u2014 single stocks, closely held interests, real estate \u2014 which commonly carry \u03c3 = 20\u201335%. This section sweeps \u03c3 from 8% to 30% and shows how win probability changes. The interaction with the CRUT's zero-floor (the trust corpus cannot go negative, unlike a leveraged benchmark) may produce asymmetric effects.\n\n### 3. Return \u00d7 Volatility Joint Surface\n\u03bc ranked #1. Its effect on win probability does not operate in isolation \u2014 it depends on \u03c3. A portfolio with \u03bc = 10% and \u03c3 = 25% may be more or less favorable to the CRUT than \u03bc = 7% and \u03c3 = 12%, depending on how volatility drag affects the comparison. A 2D heatmap with iso-Sharpe ratio contours reveals the joint structure.\n\n### 4. Student-t Return Distribution (Primary Robustness Check)\nEquity returns are empirically leptokurtic \u2014 fatter tails than log-normal. This is well documented in the financial economics literature (Mandelbrot 1963; Fama 1965; subsequent literature). The log-normal assumption understates the probability of extreme outcomes in both directions.\n\nWe replace the log-normal distribution with a **scaled Student-t distribution** with \u03bd = 5 degrees of freedom, which captures equity-like kurtosis (excess kurtosis \u2248 6 for \u03bd = 5, compared to approximately 1\u20133 observed in equity markets). We match the same \u03bc and \u03c3 as the log-normal baseline so the comparison isolates the distributional shape effect.\n\nFat tails affect the CRUT and the benchmark asymmetrically:\n- The CRUT corpus has a floor at zero (the trust cannot go below zero \u2014 it simply terminates with no remaining distributions). Fat left tails hurt the benchmark more in absolute terms because the benchmark carries the full corpus at risk.\n- However, fat right tails (large positive returns) may favor the benchmark more, since the benchmark retains the full corpus to compound.\n- The net effect on win probability is the subject of this section.\n\n### 5. Sequence of Returns Risk\nThe order of returns matters for the CRUT differently than for the benchmark. The CRUT pays out a fixed percentage of corpus each year \u2014 adverse early returns reduce the corpus from which all future distributions are calculated, producing a permanent impairment of income. The benchmark does not distribute; it simply compounds. This section splits Monte Carlo paths into early-bad/late-good and early-good/late-bad quartiles and compares win probability across sequence categories.\n\n### 6. Bear Market Stress Test\nA -30% shock in year 1 followed by normal returns tests how each strategy recovers from an immediate adverse event. This is relevant for CRUT formation during or immediately before a market downturn \u2014 a common planning scenario when clients contribute assets that have already declined in value, and a risk factor for CRUTs formed near market peaks.\n\n---\n\n## Sections and Figures\n\n1. **Path count convergence** \u2014 win probability and CI width vs. n_paths\n2. **Volatility sweep** \u2014 win probability vs. \u03c3 at four turnover levels\n3. **Return \u00d7 volatility heatmap** \u2014 joint surface with iso-Sharpe contours\n4. **Student-t robustness** \u2014 log-normal vs. Student-t comparison across \u03c3 levels\n5. **Sequence of returns risk** \u2014 win probability by return sequence quartile\n6. **Bear market stress test** \u2014 year-1 shock analysis\n\n---",
   "id": "cell-markdown-header"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell-install",
    "outputId": "7266edfb-563c-42bc-c036-2471ee36137e"
   },
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                       'numpy', 'matplotlib', 'scipy', '--quiet'])\n",
    "print('Dependencies confirmed.')"
   ],
   "id": "cell-install"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell-imports",
    "outputId": "34bb5da4-55e7-45bd-f1d7-121b1a68b99a"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, replace\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 120,\n",
    "    'font.family': 'sans-serif',\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'figure.facecolor': 'white',\n",
    "})\n",
    "\n",
    "TURNOVER_LEVELS = [0.00, 0.20, 0.40, 0.60]\n",
    "TURNOVER_LABELS = ['0% (buy-and-hold)', '20% (moderate)', '40% (active)', '60% (original baseline)']\n",
    "TURNOVER_COLORS = ['#08306b', '#2171b5', '#fd8d3c', '#d73027']\n",
    "\n",
    "print('Imports complete.')"
   ],
   "id": "cell-imports"
  },
  {
   "cell_type": "markdown",
   "id": "actuarial-ref-nb06",
   "metadata": {},
   "source": "---\n## Actuarial Valuation Engine\n\nThis notebook uses a self-contained copy of the corrected actuarial engine\nfirst established in Notebook 00 v3. The engine implements the exact\nsummation method authorized under Treas. Reg. \u00a71.664-4(e)(5)(i), as\ndocumented in:\n\n> Klaus Gottlieb, *From Myth to Math: A Tutorial on the Actuarial Valuation\n> of Charitable Remainder Unitrusts under I.R.C. \u00a7 7520* (2025) (unpublished\n> manuscript), *available at* https://ssrn.com/abstract=5924942.\n\nThe F-factor is computed per Reg. \u00a71.664-4(e)(6)(ii) (Equation 1); the\nremainder factor via the full probability-weighted summation over Table 2010CM\n(Equations 4 and 6). See Notebook 00 for the complete derivation and\nvalidation against CalCRUT.com.\n\n**NB06-specific extensions:**\n- `generate_return_paths()` extended with `dist_type` ('lognormal' or\n  'student_t') and `df` parameters for the Student-t robustness check.\n- `ScenarioParams` includes `dist_type`, `student_t_df`, and\n  `first_year_shock` for the bear market stress test.\n- `run_simulation()` accepts `precomputed_paths` for the sequence-of-returns\n  analysis where paths are sorted externally before being passed in.\n- `bootstrap_ci()` provides 95% confidence intervals on win probability\n  estimates, used in the path count convergence analysis (Section 1).\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell-engine",
    "outputId": "210e4de5-c102-4ded-d1a2-aa566945d7f7"
   },
   "outputs": [],
   "source": "# =============================================================================\n# ACTUARIAL ENGINE \u2014 self-contained copy from Notebook 00 v3\n# implements Gottlieb (2025) Equations 1\u20136 and Table 2010CM\n# Extended for NB06: Student-t returns, first_year_shock, precomputed_paths\n# =============================================================================\n\n# =============================================================================\n# ACTUARIAL ENGINE \u2014 implements Gottlieb (2025) Equations 1\u20136 and Table 2010CM\n# =============================================================================\n\nfrom typing import Dict\n\n# Table 2010CM \u2014 lx values (number living at age x, radix 100,000).\n# Prescribed mortality table under IRC \u00a77520; effective June 1, 2023.\n# Source: Treas. Reg. \u00a7\u00a720.2031-7, 25.2512-5.\nMORTALITY_2010CM: Dict[int, float] = {\n    0: 100000.0, 1: 99382.28, 2: 99341.16, 3: 99313.80, 4: 99292.72,\n    5: 99276.45, 6: 99261.55, 7: 99248.33, 8: 99236.50, 9: 99226.09,\n    10: 99217.03, 11: 99208.80, 12: 99199.98, 13: 99188.21, 14: 99170.64,\n    15: 99145.34, 16: 99111.91, 17: 99070.69, 18: 99021.50, 19: 98964.16,\n    20: 98898.61, 21: 98824.20, 22: 98741.32, 23: 98652.16, 24: 98559.87,\n    25: 98466.80, 26: 98373.71, 27: 98280.09, 28: 98185.51, 29: 98089.05,\n    30: 97989.90, 31: 97887.47, 32: 97781.58, 33: 97672.13, 34: 97559.20,\n    35: 97442.53, 36: 97321.14, 37: 97193.66, 38: 97058.84, 39: 96915.25,\n    40: 96761.20, 41: 96595.51, 42: 96416.30, 43: 96220.61, 44: 96005.41,\n    45: 95768.60, 46: 95509.98, 47: 95229.06, 48: 94923.45, 49: 94589.88,\n    50: 94225.50, 51: 93828.33, 52: 93398.01, 53: 92934.52, 54: 92438.08,\n    55: 91907.95, 56: 91342.02, 57: 90737.24, 58: 90090.97, 59: 89401.06,\n    60: 88665.95, 61: 87883.66, 62: 87051.88, 63: 86167.86, 64: 85226.77,\n    65: 84221.59, 66: 83142.34, 67: 81978.28, 68: 80728.83, 69: 79387.95,\n    70: 77957.53, 71: 76429.84, 72: 74797.63, 73: 73049.33, 74: 71177.55,\n    75: 69174.83, 76: 67044.59, 77: 64773.93, 78: 62366.05, 79: 59795.50,\n    80: 57080.84, 81: 54213.71, 82: 51205.27, 83: 48059.88, 84: 44808.51,\n    85: 41399.79, 86: 37895.25, 87: 34313.98, 88: 30700.82, 89: 27106.68,\n    90: 23586.75, 91: 20198.02, 92: 16996.17, 93: 14032.08, 94: 11348.23,\n    95: 8975.661, 96: 6931.559, 97: 5218.261, 98: 3823.642, 99: 2722.994,\n    100: 1882.108, 101: 1261.083, 102: 818.2641, 103: 513.7236,\n    104: 311.8784, 105: 183.0200, 106: 103.8046, 107: 56.91106,\n    108: 30.17214, 109: 15.47804, 110: 0.0\n}\n_MAX_AGE = 110\n\n# \u2500\u2500 Mortality helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef _lx(age: int) -> float:\n    \"\"\"lx from Table 2010CM; returns 0 for age >= 110.\"\"\"\n    if age >= _MAX_AGE:\n        return 0.0\n    return MORTALITY_2010CM.get(int(age), 0.0)\n\ndef _tqx(x: int, t: int) -> float:\n    \"\"\"Cumulative probability that a person aged x dies within t years.\n    tqx = 1 - l(x+t) / lx.  Returns 1.0 if lx = 0.\"\"\"\n    lx = _lx(x)\n    if lx == 0.0:\n        return 1.0\n    lxt = _lx(x + t) if (x + t) < _MAX_AGE else 0.0\n    return 1.0 - lxt / lx\n\ndef life_expectancy_single(age: int) -> float:\n    \"\"\"Curtate single-life expectancy from 2010CM: E[K] = sum_{t=1}^{omega} tpx.\"\"\"\n    lx = _lx(age)\n    if lx == 0.0:\n        return 0.0\n    return sum(_lx(age + t) / lx for t in range(1, _MAX_AGE - age + 1))\n\ndef life_expectancy_joint_last(age1: int, age2: int) -> float:\n    \"\"\"Curtate last-survivor expectancy from 2010CM.\n    E[T_last] = sum_{t=1}^{omega} tpxy, where tpxy = 1 - tqx * tqy\n    (Equation 5: law of complementary events).\"\"\"\n    max_t = _MAX_AGE - min(age1, age2)\n    return sum(1.0 - _tqx(age1, t) * _tqx(age2, t) for t in range(1, max_t + 1))\n\n# \u2500\u2500 Remainder factor summations \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef _remainder_single(u: float, age: int) -> float:\n    \"\"\"Single-life remainder factor via the Treasury Figure 1 summation\n    (Equation 4). Synthetic rate i\\'= u/(1-u), decay factor v\\'= 1-u.\n    R = (1 + i\\'/2) * sum_{t=0}^{omega-x} (v\\')^{t+1} * (t+1_qx - t_qx).\"\"\"\n    if u <= 0.0: return 1.0\n    if u >= 1.0: return 0.0\n    i_syn = u / (1.0 - u)\n    v = 1.0 - u\n    total, prev_tqx = 0.0, 0.0\n    for t in range(_MAX_AGE - age):\n        cur_tqx = _tqx(age, t + 1)\n        total  += (v ** (t + 1)) * (cur_tqx - prev_tqx)\n        prev_tqx = cur_tqx\n    return (1.0 + i_syn / 2.0) * total\n\ndef _remainder_two_life(u: float, age1: int, age2: int) -> float:\n    \"\"\"Two-life last-survivor remainder factor via the Treasury Figure 1\n    summation (Equation 6).\n    R = (1 + i\\'/2) * sum_{t=0}^{omega} (v\\')^{t+1} * (tpxy - t+1_pxy),\n    where tpxy = 1 - tqx * tqy  (Equation 5).\"\"\"\n    if u <= 0.0: return 1.0\n    if u >= 1.0: return 0.0\n    i_syn = u / (1.0 - u)\n    v = 1.0 - u\n    max_t = _MAX_AGE - min(age1, age2)\n    total, prev_tpxy = 0.0, 1.0\n    for t in range(max_t):\n        t1pxy  = 1.0 - _tqx(age1, t + 1) * _tqx(age2, t + 1)\n        total += (v ** (t + 1)) * (prev_tpxy - t1pxy)\n        prev_tpxy = t1pxy\n    return (1.0 + i_syn / 2.0) * total\n\n# \u2500\u2500 Deduction computation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef compute_deduction(fmv, payout_rate, rate_7520, life_type,\n                      age1=65, age2=None, term_years=20,\n                      freq=4, lag_months=0, longevity_adj=0):\n    \"\"\"\n    Compute CRUT charitable deduction per Treas. Reg. \u00a71.664-4.\n\n    F-factor (Reg \u00a71.664-4(e)(6)(ii), Equation 1):\n        F = (1/p) * i * v^(1 + lag_months/12) * (1+i)^(1/p) / ((1+i)^(1/p) - 1)\n    where v = 1/(1+i).  For standard end-of-period payments with no\n    additional lag (lag_months=0) this reproduces IRS Table F values.\n\n    longevity_adj modifies only the simulation horizon, not the deduction.\n    The deduction is fixed at trust inception per the IRS actuarial duration.\n    \"\"\"\n    i = rate_7520\n    v = 1.0 / (1.0 + i)\n    # Equation 1: F-factor\n    table_f = (1.0 / freq) * i * (v ** (1.0 + lag_months / 12.0)) *               (1.0 + i) ** (1.0 / freq) / ((1.0 + i) ** (1.0 / freq) - 1.0)\n    u = payout_rate * table_f                          # Equation 2\n\n    if life_type == 'Term of Years':\n        irs_duration = float(term_years)\n        R = max(0.0, min(1.0, (1.0 - u) ** irs_duration))   # Equation 3\n\n    elif life_type == 'Single Life':\n        irs_duration = life_expectancy_single(age1)\n        R = _remainder_single(u, age1)                        # Equation 4\n\n    else:  # Two Life\n        irs_duration = life_expectancy_joint_last(age1, age2)\n        R = _remainder_two_life(u, age1, age2)                # Equation 6\n\n    return {\n        'deduction':        fmv * R,\n        'remainder_factor': R,\n        'compliance':       R >= 0.10,\n        'irs_duration':     irs_duration,\n        'sim_horizon':      irs_duration + longevity_adj,\n        'table_f':          table_f,\n        'adjusted_payout':  u,\n    }\n\n# \u2500\u2500 Return path generator (unchanged) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef generate_return_paths(mu, sigma, n_years, n_paths, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    mu_log    = np.log(1 + mu) - 0.5 * (sigma / (1 + mu)) ** 2\n    sigma_log = sigma / (1 + mu)\n    return np.exp(np.random.normal(mu_log, sigma_log, size=(n_paths, n_years)))\n\n# \u2500\u2500 Scenario parameters \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n@dataclass\nclass ScenarioParams:\n    fmv:                float         = 1_000_000\n    basis_pct:          float         = 0.20\n    agi:                float         = 500_000\n    payout_rate:        float         = 0.06\n    life_type:          str           = 'Two Life'\n    age1:               int           = 63\n    age2:               Optional[int] = 65\n    term_years:         int           = 20\n    freq:               int           = 4\n    lag_months:         int           = 0\n    longevity_adj:      int           = 0\n    rate_7520:          float         = 0.05\n    pv_rate:            float         = 0.05\n    fed_ordinary:       float         = 0.37\n    fed_ltcg:           float         = 0.20\n    niit:               float         = 0.038\n    state_rate:         float         = 0.093\n    agi_limit_pct:      float         = 0.30\n    carryforward_years: int           = 5\n    trust_fee:          float         = 0.01\n    bench_fee:          float         = 0.01\n    turnover:           float         = 0.20\n    mu:                 float         = 0.07\n    sigma:              float         = 0.12\n    n_paths:            int           = 10_000\n    seed:               int           = 42\n\n# \u2500\u2500 Simulation engine (unchanged) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef generate_return_paths(mu, sigma, n_years, n_paths,\n                          seed=None, dist_type='lognormal', df=5):\n    \"\"\"\n    Generate annual gross return paths (1 + r_t).\n\n    dist_type = 'lognormal' (default):\n        Standard log-normal parameterization matched to arithmetic mean mu\n        and standard deviation sigma.\n\n    dist_type = 'student_t':\n        Scaled Student-t with df degrees of freedom. Parameterization: draw\n        z ~ t(df), scale to match the same mu and sigma as the log-normal case.\n          scale = sigma / sqrt(df / (df - 2))\n          r_t   = mu + scale * z\n          return = max(0.01, 1 + r_t)   (floor at -99%)\n        Preserves mu and sigma while introducing fat tails.\n        df=5: excess kurtosis = 6/(df-4) = 6.\n        Log-normal excess kurtosis at sigma=12% is approximately 0.05.\n\n    Note on paired-path design: both CRUT and benchmark use the same paths.\n    Distribution type is applied identically to both strategies.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    if dist_type == 'lognormal':\n        mu_log    = np.log(1 + mu) - 0.5 * (sigma / (1 + mu)) ** 2\n        sigma_log = sigma / (1 + mu)\n        return np.exp(np.random.normal(mu_log, sigma_log,\n                                       size=(n_paths, n_years)))\n    elif dist_type == 'student_t':\n        if df <= 2:\n            raise ValueError('df must be > 2 for finite variance')\n        t_scale = sigma / np.sqrt(df / (df - 2))\n        z       = np.random.standard_t(df, size=(n_paths, n_years))\n        r_t     = mu + t_scale * z\n        return np.maximum(0.01, 1.0 + r_t)\n    else:\n        raise ValueError(f'Unknown dist_type: {dist_type}')\n\n\n@dataclass\nclass ScenarioParams:\n    fmv:                float         = 1_000_000\n    basis_pct:          float         = 0.20\n    agi:                float         = 500_000\n    payout_rate:        float         = 0.06\n    life_type:          str           = 'Two Life'\n    age1:               int           = 63\n    age2:               Optional[int] = 65\n    term_years:         int           = 20\n    freq:               int           = 4\n    lag_months:         int           = 0\n    longevity_adj:      int           = 0\n    rate_7520:          float         = 0.05\n    pv_rate:            float         = 0.05\n    fed_ordinary:       float         = 0.37\n    fed_ltcg:           float         = 0.20\n    niit:               float         = 0.038\n    state_rate:         float         = 0.093\n    agi_limit_pct:      float         = 0.30\n    carryforward_years: int           = 5\n    trust_fee:          float         = 0.01\n    bench_fee:          float         = 0.01\n    turnover:           float         = 0.20\n    mu:                 float         = 0.07\n    sigma:              float         = 0.12\n    n_paths:            int           = 10_000\n    seed:               int           = 42\n    dist_type:          str           = 'lognormal'   # 'lognormal' or 'student_t'\n    student_t_df:       float         = 5.0           # degrees of freedom for Student-t\n    first_year_shock:   float         = 1.0           # multiplicative shock to year-1 return\n\n\ndef run_simulation(params, precomputed_paths=None):\n    \"\"\"\n    Run paired-path Monte Carlo CRUT vs. hold-liquidation benchmark.\n\n    Parameters\n    ----------\n    params : ScenarioParams\n    precomputed_paths : np.ndarray, optional\n        If provided, use these return paths instead of generating new ones.\n        Shape must be (n_paths, T). Used for sequence-of-returns analysis\n        where paths are sorted or reordered externally before being passed in.\n\n    Notes\n    -----\n    dist_type='lognormal': standard baseline, independent annual log-normal returns.\n    dist_type='student_t': fat-tailed returns, same mu/sigma, df=student_t_df.\n    first_year_shock: multiplied into year-1 return for all paths; set to 0.70\n        for a -30% bear market stress test.\n\n    \u00a77520 rate enters exclusively through compute_deduction() via the F-Factor.\n    It is not a discount rate for simulation cash flows. pv_rate is an\n    independent donor preference parameter.\n    \"\"\"\n    p       = params\n    tau_ord = p.fed_ordinary + p.state_rate\n    tau_cg  = p.fed_ltcg + p.niit + p.state_rate\n    # OBBBA: deduction benefit capped at 35% federal for top-bracket filers\n    combined_ord = min(p.fed_ordinary, 0.35) + p.state_rate\n\n    ded_res = compute_deduction(\n        fmv=p.fmv, payout_rate=p.payout_rate, rate_7520=p.rate_7520,\n        life_type=p.life_type, age1=p.age1, age2=p.age2,\n        term_years=p.term_years, freq=p.freq, lag_months=p.lag_months,\n        longevity_adj=p.longevity_adj,\n    )\n    T            = max(1, int(round(ded_res['sim_horizon'])))\n    deduction    = ded_res['deduction']\n    annual_limit = p.agi * p.agi_limit_pct\n    remaining    = deduction\n    pv_tax       = 0.0\n    for yr in range(p.carryforward_years + 1):\n        usable    = min(remaining, annual_limit)\n        if usable <= 0:\n            break\n        pv_tax   += usable * combined_ord / (1 + p.pv_rate) ** yr\n        remaining -= usable\n\n    if precomputed_paths is not None:\n        returns = precomputed_paths[:, :T]\n    else:\n        returns = generate_return_paths(\n            p.mu, p.sigma, T, p.n_paths,\n            seed=p.seed,\n            dist_type=p.dist_type,\n            df=p.student_t_df,\n        )\n        if p.first_year_shock != 1.0:\n            returns[:, 0] = returns[:, 0] * p.first_year_shock\n\n    # CRUT\n    crut_v = np.full(p.n_paths, p.fmv)\n    dists  = np.zeros((p.n_paths, T))\n    for t in range(T):\n        v           = crut_v * (1 - p.trust_fee) * returns[:, t]\n        d           = v * p.payout_rate\n        dists[:, t] = d * (1 - tau_ord)\n        crut_v      = np.maximum(0, v - d)\n    disc        = np.array([(1 + p.pv_rate) ** -(t + 1) for t in range(T)])\n    crut_wealth = (dists * disc).sum(axis=1) + pv_tax\n\n    # Benchmark \u2014 hold-liquidation\n    bench_v     = np.full(p.n_paths, p.fmv)\n    bench_basis = p.fmv * p.basis_pct\n    for t in range(T):\n        b           = bench_v * (1 - p.bench_fee) * returns[:, t]\n        gain        = np.maximum(0, b - bench_basis)\n        tax_drag    = p.turnover * gain * tau_cg\n        bench_v     = np.maximum(0, b - tax_drag)\n        bench_basis = bench_basis + p.turnover * gain * (1 - tau_cg)\n    term_gain    = np.maximum(0, bench_v - bench_basis)\n    bench_term   = bench_v - term_gain * tau_cg\n    bench_wealth = bench_term / (1 + p.pv_rate) ** T\n\n    delta = crut_wealth - bench_wealth\n    return {\n        'win_prob':     float(np.mean(delta > 0)),\n        'median_delta': float(np.median(delta)),\n        'delta_wealth': delta,\n        'crut_wealth':  crut_wealth,\n        'bench_wealth': bench_wealth,\n        'pv_tax':       pv_tax,\n        'deduction':    deduction,\n        'T':            T,\n        'params':       p,\n    }\n\n\ndef bootstrap_ci(delta_arr, n_boot=1000, ci=0.95, seed=0):\n    rng   = np.random.RandomState(seed)\n    n     = len(delta_arr)\n    boot  = [np.mean(rng.choice(delta_arr, size=n, replace=True) > 0)\n             for _ in range(n_boot)]\n    alpha = 1 - ci\n    return (float(np.percentile(boot, 100 * alpha / 2)),\n            float(np.percentile(boot, 100 * (1 - alpha / 2))))\n\n\n# --- Actuarial verification ---\nbaseline = ScenarioParams()\n_dc = compute_deduction(1_000_000, 0.06, 0.05, 'Two Life', 63, 65, freq=4)\nprint('Actuarial verification (Two Life 63/65, 6% payout, 5% \u00a77520, quarterly):')\nprint(f'  Remainder factor : {_dc[\"remainder_factor\"]:.6f}  (CalCRUT: 0.240220)')\nprint(f'  Deduction on $1M : ${_dc[\"deduction\"]:,.2f}  (CalCRUT: $240,220.39)')\nprint(f'  IRS duration     : {_dc[\"irs_duration\"]:.2f} yr')\nprint()\n\nr0 = run_simulation(baseline)\nprint('Baseline (log-normal, Two Life 63/65, 20% turnover, n=10,000):')\nprint(f'  Win probability:  {r0[\"win_prob\"]:.1%}')\nprint(f'  Median delta:     ${r0[\"median_delta\"]:,.0f}')\nprint(f'  Deduction:        ${r0[\"deduction\"]:,.0f}')\nprint(f'  PV tax benefit:   ${r0[\"pv_tax\"]:,.0f}')\nprint(f'  Sim horizon:      {r0[\"T\"]} yr')\nprint()\n\nrt = run_simulation(replace(baseline, dist_type='student_t', student_t_df=5.0))\nprint('Student-t verification (df=5, same mu/sigma):')\nprint(f'  Win probability:  {rt[\"win_prob\"]:.1%}')\nprint(f'  Median delta:     ${rt[\"median_delta\"]:,.0f}')\nprint()\nprint('Engine ready.')\n",
   "id": "cell-engine"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-markdown-fig1"
   },
   "source": "---\n## Section 1 \u2014 Path Count Convergence\n\n### Figure 1: Win Probability and CI Width vs. Number of Paths\n\nMonte Carlo results are estimates, not exact values. More paths produce more precise estimates. The question is: how many paths are needed before the estimate stabilizes?\n\n**Convergence criterion:** We consider the simulation converged when the 95% bootstrap confidence interval width falls below **2 percentage points** (i.e., \u00b1 1pp). This is the precision level at which the win probability estimate is clinically meaningful for planning purposes.\n\n**The tradeoff:** More paths mean more computation. At 10,000 paths, each simulation takes approximately 1.5\u20132.5 seconds on Colab. The full manuscript sweep (600+ simulations across NB02) takes approximately 8\u201312 minutes \u2014 acceptable given the precision gained. This figure confirms that 10,000 paths is both necessary and sufficient: the CI width falls below 2 pp well before 10,000 but meaningful noise remains at lower counts.\n\nWe run each path count level 5 times with different seeds and report the mean win probability and its variation across seeds \u2014 this shows both the point estimate stability and the seed sensitivity.",
   "id": "cell-markdown-fig1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "cell-fig1",
    "outputId": "115e229c-cea5-493f-bac3-efb532a88979"
   },
   "outputs": [],
   "source": [
    "# --- Figure 1: Path count convergence ---\n",
    "\n",
    "path_counts = [200, 500, 1000, 2000, 3000, 5000, 7500, 10000]\n",
    "n_reps      = 5   # independent seeds per path count\n",
    "\n",
    "print('Running path count convergence...')\n",
    "print(f'  {len(path_counts)} path counts x {n_reps} seeds = '\n",
    "      f'{len(path_counts)*n_reps} simulations')\n",
    "\n",
    "conv_wp   = []   # mean win prob at each path count\n",
    "conv_ci   = []   # mean CI width at each path count\n",
    "conv_std  = []   # std of win prob across seeds\n",
    "\n",
    "for n in path_counts:\n",
    "    wps, ci_widths = [], []\n",
    "    for seed in range(n_reps):\n",
    "        r = run_simulation(replace(baseline, n_paths=n, seed=seed*17))\n",
    "        lo, hi = bootstrap_ci(r['delta_wealth'], n_boot=500)\n",
    "        wps.append(r['win_prob'])\n",
    "        ci_widths.append((hi - lo) * 100)\n",
    "    conv_wp.append(np.mean(wps) * 100)\n",
    "    conv_ci.append(np.mean(ci_widths))\n",
    "    conv_std.append(np.std(wps) * 100)\n",
    "    print(f'  n={n:>6}: mean wp={conv_wp[-1]:.1f}%  '\n",
    "          f'CI width={conv_ci[-1]:.2f} pp  '\n",
    "          f'seed std={conv_std[-1]:.2f} pp')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(path_counts, conv_wp, color='#2171b5', lw=2.5, marker='o', label='Mean win probability')\n",
    "ax.fill_between(path_counts,\n",
    "                np.array(conv_wp) - np.array(conv_std),\n",
    "                np.array(conv_wp) + np.array(conv_std),\n",
    "                alpha=0.2, color='#2171b5', label='\u00b11 std across seeds')\n",
    "ax.axvline(2000, color='orange', lw=2, ls=':', label='Baseline n=2,000')\n",
    "ax.set_xlabel('Number of Simulation Paths', fontsize=11)\n",
    "ax.set_ylabel('Win Probability (%)', fontsize=11)\n",
    "ax.set_title('Figure 1a: Win Probability Convergence\\nvs. Number of Paths (5 seeds each)', fontsize=10)\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(path_counts, conv_ci, color='#d73027', lw=2.5, marker='o', label='Mean 95% CI width')\n",
    "ax2.axhline(2.0, color='gray', lw=1.5, ls='--', label='2 pp convergence threshold')\n",
    "ax2.axvline(2000, color='orange', lw=2, ls=':', label='Baseline n=2,000')\n",
    "\n",
    "# Mark where CI first drops below 2pp\n",
    "for i, (n, ci) in enumerate(zip(path_counts, conv_ci)):\n",
    "    if ci < 2.0:\n",
    "        ax2.scatter([n], [ci], color='green', s=150, zorder=5, marker='*')\n",
    "        ax2.annotate(f'First < 2pp\\n(n={n:,})',\n",
    "                     xy=(n, ci), xytext=(n*1.3, ci + 0.3),\n",
    "                     fontsize=9, color='green',\n",
    "                     arrowprops=dict(arrowstyle='->', color='green'))\n",
    "        break\n",
    "\n",
    "ax2.set_xlabel('Number of Simulation Paths', fontsize=11)\n",
    "ax2.set_ylabel('95% CI Width (pp)', fontsize=11)\n",
    "ax2.set_title('Figure 1b: Confidence Interval Width vs. Paths\\n'\n",
    "              'CI < 2 pp = clinically meaningful precision', fontsize=10)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "fig.suptitle(\n",
    "    'Figure 1: Monte Carlo Convergence Analysis\\n'\n",
    "    'Star = first path count achieving < 2 pp CI | Baseline n=2,000 shown in orange',\n",
    "    fontsize=11\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig1_convergence.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print('Figure 1 saved.')"
   ],
   "id": "cell-fig1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-markdown-fig2"
   },
   "source": [
    "---\n",
    "## Section 2 \u2014 Volatility Sweep\n",
    "\n",
    "### Figure 2: Win Probability vs. Portfolio Volatility \u03c3\n",
    "\n",
    "The baseline \u03c3 = 12% represents a diversified multi-asset portfolio. The clients most commonly advised to use a CRUT hold **concentrated positions** \u2014 single stocks, closely held businesses, or real estate \u2014 which routinely carry \u03c3 = 20\u201335%. A tech founder's single-stock position might carry \u03c3 = 40% or higher.\n",
    "\n",
    "After contributing to the CRUT, both the trust and the benchmark (in the hold scenarios) are assumed to diversify immediately. So \u03c3 for the ongoing comparison is the diversified portfolio volatility, not the concentrated position volatility. However, in the liquidation-reinvest benchmark (Notebook 02), the reinvested portfolio also diversifies.\n",
    "\n",
    "**The effect of \u03c3 on win probability operates through two channels:**\n",
    "\n",
    "1. **Volatility drag:** Higher \u03c3 reduces the geometric mean return (the arithmetic mean \u03bc is fixed in this sweep, so the geometric mean = \u03bc - \u03c3\u00b2/2 falls as \u03c3 rises). Both CRUT and benchmark suffer volatility drag, but the CRUT pays out annually \u2014 it realizes income even in down years \u2014 while the benchmark compounds the full volatile corpus.\n",
    "\n",
    "2. **Downside asymmetry:** The CRUT corpus has a practical floor at zero (a catastrophic loss year does not create negative distributions). The benchmark's unrealized gain can be wiped out in a crash, making the terminal tax liability smaller \u2014 potentially helping the benchmark. The net effect on win probability depends on whether the CRUT's distribution stream suffers more or less than the benchmark's compounding advantage under high volatility."
   ],
   "id": "cell-markdown-fig2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "cell-fig2",
    "outputId": "426edee6-4ffe-4449-a35a-f203e1e515e4"
   },
   "outputs": [],
   "source": [
    "# --- Figure 2: Volatility sweep ---\n",
    "\n",
    "sigma_vals = np.linspace(0.06, 0.32, 30)\n",
    "\n",
    "print('Running volatility sweep...')\n",
    "vol_results = {}\n",
    "for tv in TURNOVER_LEVELS:\n",
    "    wp_arr = []\n",
    "    for sigma in sigma_vals:\n",
    "        r = run_simulation(replace(baseline, sigma=sigma, turnover=tv))\n",
    "        wp_arr.append(r['win_prob'])\n",
    "    vol_results[tv] = np.array(wp_arr)\n",
    "    print(f'  {tv*100:.0f}% turnover: done')\n",
    "\n",
    "idx_base_sigma = np.argmin(np.abs(sigma_vals - 0.12))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "for tv, color, tv_label, lw in zip(\n",
    "        TURNOVER_LEVELS, TURNOVER_COLORS, TURNOVER_LABELS, [2.5,2.5,1.8,1.8]):\n",
    "    ax.plot(sigma_vals * 100, vol_results[tv] * 100,\n",
    "            color=color, lw=lw, label=f'{tv*100:.0f}% turnover')\n",
    "\n",
    "ax.axvline(12, color='orange', lw=1.5, ls=':', label='Baseline \u03c3=12%')\n",
    "ax.axhline(50, color='black',  lw=1.2, ls=':')\n",
    "\n",
    "# Shade typical concentrated position range\n",
    "ax.axvspan(20, 35, alpha=0.07, color='purple',\n",
    "           label='Concentrated position range (20\u201335%)')\n",
    "\n",
    "ax.set_xlabel('Annual Return Volatility \u03c3 (%)', fontsize=11)\n",
    "ax.set_ylabel('Win Probability (%)', fontsize=11)\n",
    "ax.set_title('Figure 2a: Win Probability vs. Volatility \u03c3\\n'\n",
    "             '\u03bc = 7% fixed | Purple = concentrated position range', fontsize=10)\n",
    "ax.legend(fontsize=8, loc='upper right')\n",
    "ax.set_xlim(sigma_vals[0]*100, sigma_vals[-1]*100)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Right panel: geometric mean drag vs sigma\n",
    "ax2 = axes[1]\n",
    "geo_means = [(1 + 0.07) * np.exp(-0.5 * (s/(1+0.07))**2) - 1 for s in sigma_vals]\n",
    "ax2.plot(sigma_vals*100, np.array(geo_means)*100, color='#1a9641', lw=2.5,\n",
    "         label='Geometric mean return (\u03bc=7% fixed)')\n",
    "ax2.axvline(12, color='orange', lw=1.5, ls=':')\n",
    "ax2.axhline(7.0, color='gray', lw=1, ls='--', label='\u03bc = 7% (arithmetic)')\n",
    "ax2.set_xlabel('Annual Return Volatility \u03c3 (%)', fontsize=11)\n",
    "ax2.set_ylabel('Geometric Mean Return (%)', fontsize=11)\n",
    "ax2.set_title('Figure 2b: Volatility Drag on Geometric Return\\n'\n",
    "              'Higher \u03c3 reduces geometric mean even with fixed arithmetic mean', fontsize=10)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.set_xlim(sigma_vals[0]*100, sigma_vals[-1]*100)\n",
    "\n",
    "fig.suptitle(\n",
    "    'Figure 2: Win Probability vs. Volatility \u03c3\\n'\n",
    "    'Arithmetic mean \u03bc held constant at 7% \u2014 volatility drag on geometric return increases with \u03c3',\n",
    "    fontsize=11\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig2_volatility_sweep.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print('Figure 2 saved.')\n",
    "print()\n",
    "print('Win probability at \u03c3=12%, 20%, 30% (20% turnover):')\n",
    "for sv, label in [(0.12, 'Baseline 12%'), (0.20, 'Conc. low 20%'), (0.30, 'Conc. high 30%')]:\n",
    "    idx = np.argmin(np.abs(sigma_vals - sv))\n",
    "    print(f'  \u03c3={sv*100:.0f}%: {vol_results[0.20][idx]:.1%}')"
   ],
   "id": "cell-fig2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-markdown-fig3"
   },
   "source": [
    "---\n",
    "## Section 3 \u2014 Return \u00d7 Volatility Joint Surface\n",
    "\n",
    "### Figure 3: Win Probability Heatmap \u2014 \u03bc \u00d7 \u03c3 with Iso-Sharpe Contours\n",
    "\n",
    "\u03bc (ranked #1) and \u03c3 (ranked #7) jointly determine the distribution of outcomes. Plotting them together reveals whether CRUT win probability is primarily determined by the level of return, the level of risk, or their ratio (the Sharpe ratio).\n",
    "\n",
    "**Iso-Sharpe contours** connect (\u03bc, \u03c3) pairs with the same Sharpe ratio (\u03bc / \u03c3, using a zero risk-free rate for simplicity). If win probability is constant along these contours, the Sharpe ratio is the single sufficient statistic. If win probability varies along a contour, the absolute levels of \u03bc and \u03c3 matter independently \u2014 which would be a meaningful finding about CRUT economics.\n",
    "\n",
    "**Economic interpretation of iso-Sharpe deviation:**\n",
    "- If higher \u03c3 at the same Sharpe hurts the CRUT (win probability falls as we move right along a Sharpe contour), it means volatility drag disproportionately harms the CRUT relative to the benchmark.\n",
    "- If higher \u03c3 at the same Sharpe helps the CRUT, volatility drag may be affecting the benchmark's deferred tax liability more than the CRUT's distribution stream."
   ],
   "id": "cell-markdown-fig3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "cell-fig3",
    "outputId": "5e11440e-34d7-4fd9-fa54-6fc9ae0c140d"
   },
   "outputs": [],
   "source": [
    "# --- Figure 3: Return x Volatility heatmap with iso-Sharpe contours ---\n",
    "\n",
    "mu_grid    = np.linspace(0.03, 0.12, 15)\n",
    "sigma_grid = np.linspace(0.06, 0.28, 15)\n",
    "\n",
    "hm_20 = np.zeros((len(sigma_grid), len(mu_grid)))\n",
    "hm_60 = np.zeros((len(sigma_grid), len(mu_grid)))\n",
    "\n",
    "print('Running return x volatility heatmap (15 x 15 x 2 = 450 simulations)...')\n",
    "for i, sg in enumerate(sigma_grid):\n",
    "    for j, mu in enumerate(mu_grid):\n",
    "        r20 = run_simulation(replace(baseline, mu=mu, sigma=sg, turnover=0.20))\n",
    "        r60 = run_simulation(replace(baseline, mu=mu, sigma=sg, turnover=0.60))\n",
    "        hm_20[i, j] = r20['win_prob']\n",
    "        hm_60[i, j] = r60['win_prob']\n",
    "    if (i+1) % 5 == 0:\n",
    "        print(f'  {i+1}/15 rows complete')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "for ax, hm, tv_label in [\n",
    "    (axes[0], hm_20, '20% Turnover'),\n",
    "    (axes[1], hm_60, '60% Turnover'),\n",
    "]:\n",
    "    im = ax.imshow(\n",
    "        hm * 100, origin='lower', aspect='auto',\n",
    "        extent=[mu_grid[0]*100, mu_grid[-1]*100,\n",
    "                sigma_grid[0]*100, sigma_grid[-1]*100],\n",
    "        cmap='RdYlBu', vmin=5, vmax=95\n",
    "    )\n",
    "    plt.colorbar(im, ax=ax, label='Win Probability (%)')\n",
    "\n",
    "    # 50% contour\n",
    "    m_g, s_g = np.meshgrid(mu_grid*100, sigma_grid*100)\n",
    "    cs50 = ax.contour(m_g, s_g, hm*100, levels=[50],\n",
    "                      colors='black', linewidths=2.5)\n",
    "    ax.clabel(cs50, fmt='50%%', fontsize=10)\n",
    "\n",
    "    # Iso-Sharpe contours (Sharpe = mu/sigma, zero rf)\n",
    "    sharpe_levels = [0.3, 0.5, 0.7, 1.0]\n",
    "    sharpe_colors = ['#fee0d2', '#fc9272', '#de2d26', '#a50f15']\n",
    "    for sh, shcolor in zip(sharpe_levels, sharpe_colors):\n",
    "        # sigma = mu / sh (line through origin in mu-sigma space)\n",
    "        mu_line = np.linspace(mu_grid[0], mu_grid[-1], 100)\n",
    "        sg_line = mu_line / sh\n",
    "        mask = (sg_line >= sigma_grid[0]) & (sg_line <= sigma_grid[-1])\n",
    "        if mask.any():\n",
    "            ax.plot(mu_line[mask]*100, sg_line[mask]*100,\n",
    "                    color=shcolor, lw=1.2, ls='--', alpha=0.8)\n",
    "            # Label at midpoint\n",
    "            mid = mask.sum() // 2\n",
    "            ax.annotate(f'SR={sh:.1f}',\n",
    "                        xy=(mu_line[mask][mid]*100, sg_line[mask][mid]*100),\n",
    "                        fontsize=7, color=shcolor, alpha=0.9)\n",
    "\n",
    "    # Mark baseline\n",
    "    ax.scatter([7], [12], color='orange', s=200, marker='*',\n",
    "               zorder=5, edgecolors='black',\n",
    "               label='Baseline (\u03bc=7%, \u03c3=12%)')\n",
    "\n",
    "    ax.set_xlabel('Expected Return \u03bc (%)', fontsize=11)\n",
    "    ax.set_ylabel('Volatility \u03c3 (%)', fontsize=11)\n",
    "    ax.set_title(f'Figure 3: \u03bc \u00d7 \u03c3 Heatmap\\n{tv_label}\\n'\n",
    "                 'Dashed = iso-Sharpe contours | Black = 50% decision boundary', fontsize=9)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "fig.suptitle(\n",
    "    'Figure 3: Win Probability \u2014 Expected Return \u00d7 Volatility Joint Surface\\n'\n",
    "    'If win probability constant along iso-Sharpe lines, Sharpe ratio is sufficient statistic\\n'\n",
    "    'Deviation from iso-Sharpe lines indicates independent \u03bc or \u03c3 effects',\n",
    "    fontsize=10\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig3_mu_sigma_heatmap.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print('Figure 3 saved.')"
   ],
   "id": "cell-fig3"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-markdown-fig4"
   },
   "source": [
    "---\n",
    "## Section 4 \u2014 Student-t Return Distribution (Primary Robustness Check)\n",
    "\n",
    "### Figure 4: Log-Normal vs. Student-t Comparison\n",
    "\n",
    "**Background on the distributional assumption:**\n",
    "\n",
    "The log-normal distribution is the workhorse of financial modeling (Black-Scholes, geometric Brownian motion), but it is well documented to understate the frequency of extreme market events. The empirical distribution of equity returns shows excess kurtosis \u2014 heavier tails than log-normal \u2014 in both daily and annual data.\n",
    "\n",
    "We model fat tails using a **scaled Student-t distribution with \u03bd = 5 degrees of freedom**. Key properties of this choice:\n",
    "\n",
    "| Property | Log-Normal (\u03c3=12%) | Student-t (\u03bd=5, \u03c3=12%) |\n",
    "|---|---|---|\n",
    "| Arithmetic mean | 7% | 7% (matched) |\n",
    "| Standard deviation | 12% | 12% (matched) |\n",
    "| Excess kurtosis | ~0.05 | 6.0 |\n",
    "| Prob(annual return < -30%) | ~0.4% | ~1.8% |\n",
    "| Prob(annual return > +50%) | ~0.4% | ~1.8% |\n",
    "\n",
    "The matching of \u03bc and \u03c3 is critical: it ensures the comparison isolates the effect of distributional *shape* (kurtosis and tail behavior) rather than differences in central tendency or scale.\n",
    "\n",
    "**Why fat tails might affect the CRUT and benchmark differently:**\n",
    "\n",
    "Fat tails increase the frequency of both large gains and large losses. The two strategies respond to these events asymmetrically:\n",
    "\n",
    "- **Large loss years:** The CRUT pays out 6% of a depleted corpus \u2014 distributions fall proportionally but the trust does not terminate. The benchmark sits on a reduced corpus with a correspondingly reduced embedded gain \u2014 its deferred tax liability shrinks, which is a *benefit* for the benchmark. Fat tails may therefore help the benchmark in loss scenarios.\n",
    "\n",
    "- **Large gain years:** The CRUT captures 6% of a large corpus in that year. The benchmark's unrealized gain grows rapidly, increasing its deferred tax liability. Large gains tend to favor the CRUT in relative terms.\n",
    "\n",
    "**The net effect is the empirical question answered by this section.** We compare win probability under log-normal and Student-t across \u03c3 levels (since fat-tail effects are more pronounced at higher \u03c3) and across turnover levels."
   ],
   "id": "cell-markdown-fig4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cell-fig4",
    "outputId": "f95c0862-8f8e-4da9-9896-f95c681a3af8"
   },
   "outputs": [],
   "source": [
    "# --- Figure 4: Log-normal vs. Student-t primary robustness check ---\n",
    "\n",
    "sigma_rob  = np.linspace(0.06, 0.28, 20)\n",
    "df_levels  = [5, 10, 20]  # degrees of freedom: fat, moderate, near-normal\n",
    "df_labels  = ['df=5 (fat tails)', 'df=10 (moderate)', 'df=20 (near-normal)']\n",
    "df_colors  = ['#d73027', '#fd8d3c', '#fdae6b']\n",
    "\n",
    "print('Running Student-t robustness check...')\n",
    "print(f'  {len(sigma_rob)} sigma levels x (1 LN + {len(df_levels)} t-dist) x 2 turnover = '\n",
    "      f'{len(sigma_rob)*( 1 + len(df_levels))*2} simulations')\n",
    "\n",
    "# Log-normal baseline\n",
    "wp_ln_20 = []\n",
    "wp_ln_60 = []\n",
    "for sg in sigma_rob:\n",
    "    wp_ln_20.append(run_simulation(replace(baseline, sigma=sg, turnover=0.20,\n",
    "                                           dist_type='lognormal'))['win_prob'])\n",
    "    wp_ln_60.append(run_simulation(replace(baseline, sigma=sg, turnover=0.60,\n",
    "                                           dist_type='lognormal'))['win_prob'])\n",
    "wp_ln_20 = np.array(wp_ln_20) * 100\n",
    "wp_ln_60 = np.array(wp_ln_60) * 100\n",
    "\n",
    "# Student-t alternatives\n",
    "t_results_20 = {}  # df -> wp array at 20% turnover\n",
    "t_results_60 = {}  # df -> wp array at 60% turnover\n",
    "for df in df_levels:\n",
    "    arr_20, arr_60 = [], []\n",
    "    for sg in sigma_rob:\n",
    "        arr_20.append(run_simulation(replace(baseline, sigma=sg, turnover=0.20,\n",
    "                                             dist_type='student_t',\n",
    "                                             student_t_df=float(df)))['win_prob'])\n",
    "        arr_60.append(run_simulation(replace(baseline, sigma=sg, turnover=0.60,\n",
    "                                             dist_type='student_t',\n",
    "                                             student_t_df=float(df)))['win_prob'])\n",
    "    t_results_20[df] = np.array(arr_20) * 100\n",
    "    t_results_60[df] = np.array(arr_60) * 100\n",
    "    print(f'  df={df}: done')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "sg_pct = sigma_rob * 100\n",
    "idx_base_sg = np.argmin(np.abs(sigma_rob - 0.12))\n",
    "\n",
    "# Top row: 20% and 60% turnover win probability\n",
    "for ax, wp_ln, t_res, tv_label in [\n",
    "    (axes[0, 0], wp_ln_20, t_results_20, '20% Turnover'),\n",
    "    (axes[0, 1], wp_ln_60, t_results_60, '60% Turnover'),\n",
    "]:\n",
    "    ax.plot(sg_pct, wp_ln, color='#2171b5', lw=3, label='Log-normal (baseline)')\n",
    "    for df, label, color in zip(df_levels, df_labels, df_colors):\n",
    "        ax.plot(sg_pct, t_res[df], color=color, lw=2, ls='--', label=f'Student-t {label}')\n",
    "    ax.axvline(12, color='orange', lw=1.5, ls=':', label='Baseline \u03c3=12%')\n",
    "    ax.axhline(50, color='black',  lw=1.2, ls=':')\n",
    "    ax.axvspan(20, 28, alpha=0.06, color='purple',\n",
    "               label='Concentrated position \u03c3 range')\n",
    "    ax.set_xlabel('Volatility \u03c3 (%)', fontsize=10)\n",
    "    ax.set_ylabel('Win Probability (%)', fontsize=10)\n",
    "    ax.set_title(f'Win Probability: Log-Normal vs. Student-t\\n{tv_label}', fontsize=10)\n",
    "    ax.legend(fontsize=7.5, loc='upper right')\n",
    "    ax.set_xlim(sg_pct[0], sg_pct[-1])\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "# Bottom row: difference (Student-t minus Log-normal) \u2014 the \"fat tail premium\"\n",
    "for ax, wp_ln, t_res, tv_label in [\n",
    "    (axes[1, 0], wp_ln_20, t_results_20, '20% Turnover \u2014 Fat Tail Premium'),\n",
    "    (axes[1, 1], wp_ln_60, t_results_60, '60% Turnover \u2014 Fat Tail Premium'),\n",
    "]:\n",
    "    for df, label, color in zip(df_levels, df_labels, df_colors):\n",
    "        diff = t_res[df] - wp_ln\n",
    "        ax.plot(sg_pct, diff, color=color, lw=2, label=f'Student-t {label}')\n",
    "        ax.fill_between(sg_pct, diff, 0,\n",
    "                        where=(diff > 0), alpha=0.08, color='green')\n",
    "        ax.fill_between(sg_pct, diff, 0,\n",
    "                        where=(diff < 0), alpha=0.08, color='red')\n",
    "    ax.axhline(0, color='black', lw=1.5)\n",
    "    ax.axvline(12, color='orange', lw=1.5, ls=':')\n",
    "    ax.axvspan(20, 28, alpha=0.06, color='purple')\n",
    "    ax.set_xlabel('Volatility \u03c3 (%)', fontsize=10)\n",
    "    ax.set_ylabel('Win Prob Difference: Student-t minus Log-Normal (pp)', fontsize=9)\n",
    "    ax.set_title(f'{tv_label}\\nGreen = fat tails favor CRUT | Red = fat tails hurt CRUT', fontsize=10)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_xlim(sg_pct[0], sg_pct[-1])\n",
    "\n",
    "fig.suptitle(\n",
    "    'Figure 4: Log-Normal vs. Student-t Return Distribution \u2014 Primary Robustness Check\\n'\n",
    "    '\u03bc = 7%, \u03c3 matched across distributions | df=5: excess kurtosis=6 | df=20: near-normal\\n'\n",
    "    'Bottom row: difference (positive = fat tails favor CRUT vs log-normal baseline)',\n",
    "    fontsize=11\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig4_student_t_robustness.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print('Figure 4 saved.')\n",
    "print()\n",
    "# Summary table at baseline sigma\n",
    "print('Win probability at baseline \u03c3=12%:')\n",
    "print(f'{\"Distribution\":<25} {\"20% turnover\":>14} {\"60% turnover\":>14}')\n",
    "print('-' * 56)\n",
    "print(f'{\"Log-normal\":<25} {wp_ln_20[idx_base_sg]:>13.1f}%  {wp_ln_60[idx_base_sg]:>13.1f}%')\n",
    "for df, label in zip(df_levels, df_labels):\n",
    "    print(f'{\"Student-t \" + label:<25} '\n",
    "          f'{t_results_20[df][idx_base_sg]:>13.1f}%  '\n",
    "          f'{t_results_60[df][idx_base_sg]:>13.1f}%')"
   ],
   "id": "cell-fig4"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-markdown-fig5"
   },
   "source": "---\n## Section 5 \u2014 Sequence of Returns Risk\n\n### Figure 5: Win Probability by Return Sequence Category\n\nThe order of returns matters for strategies that distribute or consume assets over time. The CRUT pays 6% of corpus annually \u2014 a bad early return permanently reduces the corpus base, impairing all future distributions. The benchmark holds and compounds \u2014 a bad early return reduces the corpus but the full recovery potential is retained.\n\n**This asymmetry is well known in retirement income planning** (\"sequence of returns risk\" or SORR) but has not been examined in the CRUT context. The question here is whether the CRUT is more or less exposed to SORR than the benchmark.\n\n**Method:** We generate return paths and sort them by the average return in the **first third** of the simulation horizon (years 1\u20138 of 25). We split into quartiles:\n\n- **Q1 (Early Bad):** Worst 25% of first-third returns. Early losses, potential recovery later.\n- **Q2 (Early Moderate-Bad):** 25th\u201350th percentile of first-third returns.\n- **Q3 (Early Moderate-Good):** 50th\u201375th percentile of first-third returns.\n- **Q4 (Early Good):** Best 25% of first-third returns. Early gains, potential mean reversion later.\n\nWithin each quartile, we compute win probability. If the CRUT is more exposed to SORR, win probability should be much lower in Q1 than Q4. If both strategies are equally exposed, win probability should be roughly constant across quartiles.",
   "id": "cell-markdown-fig5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "cell-fig5",
    "outputId": "95cea9cf-b776-4b99-a5da-09150ac0ef39"
   },
   "outputs": [],
   "source": [
    "# --- Figure 5: Sequence of returns risk ---\n",
    "\n",
    "print('Running sequence of returns analysis...')\n",
    "\n",
    "N_PATHS = 4000   # larger sample for stable quartile statistics\n",
    "T_full  = run_simulation(baseline)['T']\n",
    "early_cutoff = T_full // 3   # first third of horizon\n",
    "\n",
    "# Generate a large set of paths\n",
    "np.random.seed(RANDOM_SEED)\n",
    "all_paths = generate_return_paths(baseline.mu, baseline.sigma,\n",
    "                                   T_full, N_PATHS, seed=RANDOM_SEED)\n",
    "\n",
    "# Sort by average return in the first third\n",
    "early_avg = all_paths[:, :early_cutoff].prod(axis=1) ** (1/early_cutoff)  # geo mean\n",
    "sort_idx  = np.argsort(early_avg)\n",
    "quartile_size = N_PATHS // 4\n",
    "quartiles = {\n",
    "    'Q1 (Early Bad)':         sort_idx[:quartile_size],\n",
    "    'Q2 (Early Mod-Bad)':     sort_idx[quartile_size:2*quartile_size],\n",
    "    'Q3 (Early Mod-Good)':    sort_idx[2*quartile_size:3*quartile_size],\n",
    "    'Q4 (Early Good)':        sort_idx[3*quartile_size:],\n",
    "}\n",
    "q_colors = ['#d73027', '#fd8d3c', '#74c476', '#006d2c']\n",
    "\n",
    "# Compute win probability within each quartile for four turnover levels\n",
    "sorr_results = {}  # turnover -> {quartile_name -> wp}\n",
    "for tv in TURNOVER_LEVELS:\n",
    "    sorr_results[tv] = {}\n",
    "    for qname, qidx in quartiles.items():\n",
    "        q_paths = all_paths[qidx, :]\n",
    "        q_p     = replace(baseline, n_paths=len(qidx), turnover=tv)\n",
    "        r       = run_simulation(q_p, precomputed_paths=q_paths)\n",
    "        sorr_results[tv][qname] = r['win_prob']\n",
    "\n",
    "# Print SORR summary\n",
    "print(f'  Simulation horizon: {T_full} yr  |  Early period: years 1-{early_cutoff}')\n",
    "print()\n",
    "print('Win probability by quartile and turnover:')\n",
    "print(f'{\"Quartile\":<24}', end='')\n",
    "for tv in TURNOVER_LEVELS:\n",
    "    print(f'{tv*100:.0f}%tv'.rjust(10), end='')\n",
    "print()\n",
    "print('-' * 64)\n",
    "for qname in quartiles:\n",
    "    print(f'{qname:<24}', end='')\n",
    "    for tv in TURNOVER_LEVELS:\n",
    "        print(f'{sorr_results[tv][qname]*100:>9.1f}%', end='')\n",
    "    print()\n",
    "\n",
    "# Figure 5\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "x = np.arange(4)\n",
    "q_names   = list(quartiles.keys())\n",
    "width     = 0.18\n",
    "offsets   = np.linspace(-(len(TURNOVER_LEVELS)-1)/2*width,\n",
    "                         (len(TURNOVER_LEVELS)-1)/2*width, len(TURNOVER_LEVELS))\n",
    "\n",
    "ax = axes[0]\n",
    "for tv, tv_color, offset in zip(TURNOVER_LEVELS, TURNOVER_COLORS, offsets):\n",
    "    vals = [sorr_results[tv][q]*100 for q in q_names]\n",
    "    bars = ax.bar(x + offset, vals, width, color=tv_color, alpha=0.85,\n",
    "                  label=f'{tv*100:.0f}%')\n",
    "    for bar, val in zip(bars, vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, val + 0.3,\n",
    "                f'{val:.1f}%', ha='center', va='bottom', fontsize=6.5,\n",
    "                color=tv_color, fontweight='bold')\n",
    "ax.axhline(50, color='black', lw=1.5, ls='--', label='50% threshold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(q_names, fontsize=8.5)\n",
    "ax.set_ylabel('Win Probability (%)', fontsize=11)\n",
    "ax.set_ylim(0, 90)\n",
    "ax.set_title('Figure 5a: Win Probability by Return Sequence Quartile\\n'\n",
    "             'Quartiles defined by average return in first third of horizon', fontsize=10)\n",
    "ax.legend(title='Turnover', fontsize=8)\n",
    "\n",
    "# Right panel: SORR slope \u2014 Q4 minus Q1 win probability\n",
    "ax2 = axes[1]\n",
    "sorr_slope = [(sorr_results[tv]['Q4 (Early Good)'] -\n",
    "               sorr_results[tv]['Q1 (Early Bad)']) * 100\n",
    "              for tv in TURNOVER_LEVELS]\n",
    "colors_slope = ['#1a9641' if v > 0 else '#d73027' for v in sorr_slope]\n",
    "bars2 = ax2.bar([f'{tv*100:.0f}%' for tv in TURNOVER_LEVELS],\n",
    "                sorr_slope, color=colors_slope, alpha=0.85)\n",
    "for bar, val in zip(bars2, sorr_slope):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2,\n",
    "             val + 0.3 if val >= 0 else val - 1.5,\n",
    "             f'{val:+.1f} pp', ha='center', va='bottom',\n",
    "             fontsize=10, fontweight='bold')\n",
    "ax2.axhline(0, color='black', lw=1.5)\n",
    "ax2.set_xlabel('Annual Portfolio Turnover', fontsize=11)\n",
    "ax2.set_ylabel('Q4 minus Q1 Win Probability (pp)', fontsize=11)\n",
    "ax2.set_title('Figure 5b: Sequence Sensitivity\\n'\n",
    "              'Positive = CRUT wins more with early good returns\\n'\n",
    "              'Near zero = CRUT not especially sensitive to sequence', fontsize=10)\n",
    "\n",
    "fig.suptitle(\n",
    "    'Figure 5: Sequence of Returns Risk\\n'\n",
    "    'Paths sorted by early-period performance | If CRUT highly SORR-sensitive, Q1 much lower than Q4',\n",
    "    fontsize=11\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig5_sequence_risk.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print('Figure 5 saved.')"
   ],
   "id": "cell-fig5"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-markdown-fig6"
   },
   "source": [
    "---\n",
    "## Section 6 \u2014 Bear Market Stress Test\n",
    "\n",
    "### Figure 6: Year-1 Shock Analysis\n",
    "\n",
    "A bear market stress test imposes a specified return shock in year 1 and allows normal return processes from year 2 onward. This tests resilience to an adverse event immediately after CRUT formation \u2014 a practical scenario since CRUTs are often formed during periods of elevated asset values (when the contributed asset has the most appreciation), which may coincide with market peaks.\n",
    "\n",
    "**The year-1 shock affects the two strategies differently:**\n",
    "\n",
    "- **CRUT:** A large year-1 loss reduces the corpus permanently. Year-1 distributions are small (based on the post-loss corpus). The deduction was fixed at inception based on the pre-loss FMV \u2014 it is not affected by year-1 performance. The CRUT's recovery depends on subsequent returns compounding a smaller base.\n",
    "\n",
    "- **Benchmark:** A large year-1 loss reduces the benchmark corpus proportionally. The embedded capital gain shrinks (or may become a loss), reducing the deferred tax liability. This is actually a *benefit* for the benchmark \u2014 the embedded gain that was generating potential tax drag is partially or fully eliminated. A -30% shock on a $1M asset with $800K embedded gain at 33.1% LTCG would reduce the asset to $700K with the gain reduced to $500K (deferred tax now $165K vs. original $265K).\n",
    "\n",
    "The stress test therefore pits the CRUT's deduction-at-inception advantage (unaffected by year-1 performance) against the benchmark's automatic gain compression in a loss year.\n",
    "\n",
    "We sweep year-1 shocks from -40% to +20% at all four turnover levels."
   ],
   "id": "cell-markdown-fig6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "cell-fig6",
    "outputId": "298c06fb-2a60-459d-d96e-daf6ece28341"
   },
   "outputs": [],
   "source": [
    "# --- Figure 6: Bear market stress test ---\n",
    "# Year-1 shock sweep from -40% to +20%.\n",
    "# first_year_shock is a multiplicative factor on the year-1 return:\n",
    "#   shock = 0.70 -> year-1 gross return multiplied by 0.70\n",
    "#   If underlying year-1 return was +7% (gross 1.07), after shock: 1.07 * 0.70 = 0.749 (-25.1%)\n",
    "#   If underlying year-1 return was -5% (gross 0.95), after shock: 0.95 * 0.70 = 0.665 (-33.5%)\n",
    "# To impose a DETERMINISTIC year-1 return of exactly r_shock, we override year-1\n",
    "# returns for all paths. This is a cleaner stress test interpretation.\n",
    "\n",
    "shock_returns = np.linspace(-0.40, 0.20, 25)  # year-1 return (not gross)\n",
    "\n",
    "print('Running bear market stress test...')\n",
    "\n",
    "shock_results = {}\n",
    "for tv in TURNOVER_LEVELS:\n",
    "    wp_arr = []\n",
    "    for shock_r in shock_returns:\n",
    "        # Generate paths normally; override year 1 for all paths\n",
    "        T_run = run_simulation(baseline)['T']\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        paths = generate_return_paths(baseline.mu, baseline.sigma,\n",
    "                                       T_run, baseline.n_paths,\n",
    "                                       seed=RANDOM_SEED)\n",
    "        # Override year 1: set gross return = 1 + shock_r for ALL paths\n",
    "        paths[:, 0] = 1.0 + shock_r\n",
    "        r = run_simulation(replace(baseline, turnover=tv, n_paths=baseline.n_paths),\n",
    "                           precomputed_paths=paths)\n",
    "        wp_arr.append(r['win_prob'])\n",
    "    shock_results[tv] = np.array(wp_arr)\n",
    "    print(f'  {tv*100:.0f}% turnover: done')\n",
    "\n",
    "shock_pct    = shock_returns * 100\n",
    "idx_baseline = np.argmin(np.abs(shock_returns - 0.07))  # close to baseline expected return\n",
    "idx_zero     = np.argmin(np.abs(shock_returns - 0.0))\n",
    "idx_m30      = np.argmin(np.abs(shock_returns - (-0.30)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "for tv, color, tv_label, lw in zip(\n",
    "        TURNOVER_LEVELS, TURNOVER_COLORS, TURNOVER_LABELS, [2.5,2.5,1.8,1.8]):\n",
    "    ax.plot(shock_pct, shock_results[tv]*100,\n",
    "            color=color, lw=lw, label=f'{tv*100:.0f}% turnover')\n",
    "\n",
    "ax.axhline(50, color='black', lw=1.2, ls=':')\n",
    "ax.axvline(0,  color='gray',  lw=1.2, ls='--', label='No year-1 shock (0%)')\n",
    "ax.axvline(-30, color='red', lw=1.5, ls=':', label='-30% (bear market threshold)')\n",
    "ax.axvspan(-40, 0, alpha=0.04, color='red')    # loss region\n",
    "ax.axvspan(0, 20, alpha=0.04, color='green')   # gain region\n",
    "\n",
    "ax.set_xlabel('Year-1 Return (deterministic, all paths)', fontsize=11)\n",
    "ax.set_ylabel('Win Probability (%)', fontsize=11)\n",
    "ax.set_title('Figure 6a: Win Probability vs. Year-1 Return Shock\\n'\n",
    "             'Years 2+ follow normal log-normal process | '\n",
    "             'Year-1 return imposed identically on all paths', fontsize=10)\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Right panel: at -30% shock, compare to no-shock baseline\n",
    "ax2 = axes[1]\n",
    "shock_impact = [(shock_results[tv][idx_m30] - shock_results[tv][idx_zero]) * 100\n",
    "                for tv in TURNOVER_LEVELS]\n",
    "colors_impact= ['#d73027' if v < 0 else '#1a9641' for v in shock_impact]\n",
    "bars = ax2.bar([f'{tv*100:.0f}%' for tv in TURNOVER_LEVELS],\n",
    "               shock_impact, color=colors_impact, alpha=0.85)\n",
    "for bar, val in zip(bars, shock_impact):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2,\n",
    "             val - 1.5 if val < 0 else val + 0.3,\n",
    "             f'{val:+.1f} pp', ha='center', va='top' if val < 0 else 'bottom',\n",
    "             fontsize=10, fontweight='bold')\n",
    "ax2.axhline(0, color='black', lw=1.5)\n",
    "\n",
    "# Annotate with actual win prob values\n",
    "for i, (tv, impact) in enumerate(zip(TURNOVER_LEVELS, shock_impact)):\n",
    "    wp_no_shock = shock_results[tv][idx_zero] * 100\n",
    "    wp_shock    = shock_results[tv][idx_m30]  * 100\n",
    "    ax2.annotate(f'{wp_no_shock:.1f}%\u2192{wp_shock:.1f}%',\n",
    "                 xy=(i, impact/2),\n",
    "                 ha='center', va='center',\n",
    "                 fontsize=7.5, color='white', fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Annual Portfolio Turnover', fontsize=11)\n",
    "ax2.set_ylabel('Win Prob Change from -30%% Year-1 Shock (pp)', fontsize=11)\n",
    "ax2.set_title('Figure 6b: Impact of -30%% Year-1 Bear Market\\n'\n",
    "              'vs. No-Shock Baseline | Labels show before\u2192after win prob', fontsize=10)\n",
    "\n",
    "fig.suptitle(\n",
    "    'Figure 6: Bear Market Stress Test \u2014 Year-1 Return Shock\\n'\n",
    "    'CRUT deduction fixed at inception; benchmark embedded gain shrinks with loss \u2014 '\n",
    "    'asymmetric exposure',\n",
    "    fontsize=11\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig6_stress_test.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print('Figure 6 saved.')"
   ],
   "id": "cell-fig6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell-final",
    "outputId": "af588aa3-b9c9-4f3d-f304-11cdded003c6"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 65)\n",
    "print('NOTEBOOK 06 \u2014 RETURN MODEL ROBUSTNESS: KEY FINDINGS')\n",
    "print('=' * 65)\n",
    "print()\n",
    "\n",
    "print('1. Path count convergence (Figure 1):')\n",
    "for n, ci in zip(path_counts, conv_ci):\n",
    "    flag = ' <-- 2pp threshold' if ci < 2.0 and (path_counts.index(n) == 0 or\n",
    "                                                    conv_ci[path_counts.index(n)-1] >= 2.0) else ''\n",
    "    print(f'   n={n:>6}: CI width = {ci:.2f} pp{flag}')\n",
    "print()\n",
    "\n",
    "print('2. Volatility sweep (Figure 2) \u2014 win prob at 20% turnover:')\n",
    "for sv, label in [(0.12,'\u03c3=12% (baseline)'), (0.20,'\u03c3=20%'), (0.28,'\u03c3=28%')]:\n",
    "    idx = np.argmin(np.abs(sigma_vals - sv))\n",
    "    print(f'   {label}: {vol_results[0.20][idx]:.1%}')\n",
    "print()\n",
    "\n",
    "print('3. Student-t robustness (Figure 4) at baseline \u03c3=12%, 20% turnover:')\n",
    "print(f'   Log-normal:       {wp_ln_20[idx_base_sg]:.1f}%')\n",
    "for df, label in zip(df_levels, df_labels):\n",
    "    print(f'   Student-t {label}: {t_results_20[df][idx_base_sg]:.1f}%  '\n",
    "          f'(diff={t_results_20[df][idx_base_sg]-wp_ln_20[idx_base_sg]:+.1f} pp)')\n",
    "print()\n",
    "\n",
    "print('4. Sequence of returns risk (Figure 5) at 20% turnover:')\n",
    "for qname in quartiles:\n",
    "    print(f'   {qname}: {sorr_results[0.20][qname]:.1%}')\n",
    "slope_20 = sorr_results[0.20]['Q4 (Early Good)'] - sorr_results[0.20]['Q1 (Early Bad)']\n",
    "print(f'   Q4-Q1 range: {slope_20*100:.1f} pp')\n",
    "print()\n",
    "\n",
    "print('5. Bear market stress test (Figure 6) \u2014 -30%% year-1 shock:')\n",
    "for tv, tv_label in zip([0.20, 0.60], ['20% turnover', '60% turnover']):\n",
    "    base = shock_results[tv][idx_zero] * 100\n",
    "    after= shock_results[tv][idx_m30]  * 100\n",
    "    print(f'   {tv_label}: {base:.1f}% \u2192 {after:.1f}% ({after-base:+.1f} pp)')\n",
    "print()\n",
    "\n",
    "import os\n",
    "figures = ['fig1_convergence.png', 'fig2_volatility_sweep.png',\n",
    "           'fig3_mu_sigma_heatmap.png', 'fig4_student_t_robustness.png',\n",
    "           'fig5_sequence_risk.png', 'fig6_stress_test.png']\n",
    "print('Figure completion check:')\n",
    "for f in figures:\n",
    "    print(f\"  {'OK' if os.path.exists(f) else 'MISSING'} {f}\")\n",
    "print()\n",
    "print('Notebook 06 complete. All six robustness checks finished.')\n",
    "print('Manuscript analysis series: NB00-NB06 complete.')"
   ],
   "id": "cell-final"
  }
 ]
}
